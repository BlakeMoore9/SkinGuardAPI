{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c2cb87",
   "metadata": {},
   "source": [
    "## Training Model on Train Images\n",
    "**Data Preparation:**<br>\n",
    "- Use ImageDataGenerator for real-time data augmentation and preprocessing.\n",
    "- Split the dataset into training and validation subsets.\n",
    "\n",
    "**Model Configuration:**<br>\n",
    "- Load a pre-trained ResNet-50 model as the base.\n",
    "- Add a custom head (layers) to the base model for the specific classification task.\n",
    "- Freeze the layers of the base model to retain their pre-trained weights.\n",
    "\n",
    "**Handling Class Imbalance:**<br>\n",
    "- Calculate class weights to give more importance to under-represented classes during training.\n",
    "\n",
    "**Model Compilation:**<br>\n",
    "- Use the Adam optimizer.\n",
    "- Set the loss function as categorical_crossentropy suitable for multi-class classification.\n",
    "- Track AUC, Precision, and Recall as metrics.\n",
    "\n",
    "**Training Callbacks:**<br>\n",
    "- arlyStopping: Stop training early if validation loss doesn't improve for a set number of epochs.\n",
    "- ModelCheckpoint: Save the best model weights based on validation loss.\n",
    "- ReduceLROnPlateau: Reduce learning rate when validation loss plateaus.\n",
    "- TensorBoard: Enable visualization of training metrics and model profiling.\n",
    "\n",
    "**Model Training:**<br>\n",
    "- Train the model for a set number of epochs or until early stopping criteria are met.\n",
    "- Use the generated data from the ImageDataGenerator.\n",
    "- Apply class weights to handle class imbalance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e19262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.utils import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0180d3",
   "metadata": {},
   "source": [
    "## Define & Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bd9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6009 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for training with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # preprocess for ResNet-50\n",
    "    rotation_range=20,  # Augmentation\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "# Data generator for validation without data augmentation\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Data generator for testing without data augmentation\n",
    "# Note: This is technically the same as val_datagen, but for clarity, defined it separately.\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/cancer/organized/train',\n",
    "    target_size=(224, 224),  # just for clarity and safety, resizing is already done\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # labels are strings, will be one-hot-encoded\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    'D:/cancer/organized/validation',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Testing data generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'D:/cancer/organized/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df383c",
   "metadata": {},
   "source": [
    "## Load ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude output layer, initialized with imagenet weights instead of random to converge faster\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4e1ff",
   "metadata": {},
   "source": [
    "## Add custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121d2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output tensor of the base model (ResNet-50 in your case). \n",
    "# This will be the starting point for our custom head.\n",
    "x = base_model.output\n",
    "\n",
    "# Add a Global Average Pooling (GAP) layer. This layer will average the spatial dimensions \n",
    "# (height and width) of the input tensor, resulting in a tensor of shape (batch_size, channels).\n",
    "# It's a way to reduce the spatial dimensions while keeping the depth (channels).\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a Dense (fully connected) layer with 2048 neurons and ReLU activation.\n",
    "# This will learn to make high-level decisions based on the features extracted by the previous layers.\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "\n",
    "# Add a Batch Normalization layer. This layer will normalize the activations of the previous layer \n",
    "# (the Dense layer) to have a mean of 0 and a standard deviation of 1. \n",
    "# This can help in stabilizing and speeding up the training process.\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Add the final Dense layer with as many neurons as there are classes in the dataset.\n",
    "# The softmax activation function ensures the output values are in the range [0, 1] and sum to 1, \n",
    "# making them interpretable as class probabilities.\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model using the base model's input and our custom head's output.\n",
    "# This connects the base model and the custom head into a single model that we can train.\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786f671",
   "metadata": {},
   "source": [
    "## Freeze Base Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b945fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze layers of the ResNet-50 model to use it as feature extractor\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8eee9f",
   "metadata": {},
   "source": [
    "## Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7192e567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.379737609329446,\n",
       " 1: 2.787105751391466,\n",
       " 2: 1.3006493506493506,\n",
       " 3: 12.440993788819876,\n",
       " 4: 1.2850727117194183,\n",
       " 5: 0.21338020666879728,\n",
       " 6: 10.099159663865546}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember:passing the arguments positionally did not work (wrong order), had to use keyword args\n",
    "# Extract unique classes and corresponding labels\n",
    "classes = np.unique(train_generator.classes)\n",
    "y_integers = train_generator.labels\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                     classes=classes, \n",
    "                                     y=y_integers)\n",
    "\n",
    "# Convert the class weights to a dictionary format\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4014716",
   "metadata": {},
   "source": [
    "## Metrics for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4501f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ec579",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66552c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25e3d8",
   "metadata": {},
   "source": [
    "## TensorBoard Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e51b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e6285",
   "metadata": {},
   "source": [
    "##  Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c18295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory name\n",
    "checkpoint_dir = 'checkpoints'\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0b1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    patience=10, \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'best_weights.h5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_format='h5'  # explicitly set to 'h5' format\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_auc', \n",
    "    factor=0.2, \n",
    "    patience=5, \n",
    "    min_lr=1e-6, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b209b7e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1345076",
   "metadata": {},
   "source": [
    "# find out names of metrics to see hoe TF named val Auc metric, as it does not seem to find it ith val_auc\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09cb181a",
   "metadata": {},
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db1eacf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 2.1790 - auc: 0.7777 - precision: 0.4742 - recall: 0.4076\n",
      "Epoch 1: val_auc improved from -inf to 0.91347, saving model to checkpoints\\best_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JasmindelSolar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 298s 2s/step - loss: 2.1790 - auc: 0.7777 - precision: 0.4742 - recall: 0.4076 - val_loss: 1.1813 - val_auc: 0.9135 - val_precision: 0.6983 - val_recall: 0.6336 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.1976 - auc: 0.8878 - precision: 0.6452 - recall: 0.5290\n",
      "Epoch 2: val_auc did not improve from 0.91347\n",
      "187/187 [==============================] - 302s 2s/step - loss: 1.1976 - auc: 0.8878 - precision: 0.6452 - recall: 0.5290 - val_loss: 3.6267 - val_auc: 0.6700 - val_precision: 0.2855 - val_recall: 0.2525 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.0484 - auc: 0.9066 - precision: 0.6775 - recall: 0.5606\n",
      "Epoch 3: val_auc improved from 0.91347 to 0.91983, saving model to checkpoints\\best_weights.h5\n",
      "187/187 [==============================] - 301s 2s/step - loss: 1.0484 - auc: 0.9066 - precision: 0.6775 - recall: 0.5606 - val_loss: 1.0554 - val_auc: 0.9198 - val_precision: 0.7160 - val_recall: 0.6190 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.9805 - auc: 0.9216 - precision: 0.7065 - recall: 0.5831\n",
      "Epoch 4: val_auc did not improve from 0.91983\n",
      "187/187 [==============================] - 292s 2s/step - loss: 0.9805 - auc: 0.9216 - precision: 0.7065 - recall: 0.5831 - val_loss: 1.1514 - val_auc: 0.9050 - val_precision: 0.6776 - val_recall: 0.5731 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.9151 - auc: 0.9259 - precision: 0.7143 - recall: 0.6000\n",
      "Epoch 5: val_auc did not improve from 0.91983\n",
      "187/187 [==============================] - 304s 2s/step - loss: 0.9151 - auc: 0.9259 - precision: 0.7143 - recall: 0.6000 - val_loss: 1.4138 - val_auc: 0.8719 - val_precision: 0.5998 - val_recall: 0.5121 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.8790 - auc: 0.9282 - precision: 0.7074 - recall: 0.5934\n",
      "Epoch 6: val_auc did not improve from 0.91983\n",
      "187/187 [==============================] - 304s 2s/step - loss: 0.8790 - auc: 0.9282 - precision: 0.7074 - recall: 0.5934 - val_loss: 1.2728 - val_auc: 0.8942 - val_precision: 0.6220 - val_recall: 0.5423 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.8621 - auc: 0.9339 - precision: 0.7279 - recall: 0.6207\n",
      "Epoch 7: val_auc did not improve from 0.91983\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "187/187 [==============================] - 304s 2s/step - loss: 0.8621 - auc: 0.9339 - precision: 0.7279 - recall: 0.6207 - val_loss: 1.2420 - val_auc: 0.8986 - val_precision: 0.6145 - val_recall: 0.5383 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.6699 - auc: 0.9512 - precision: 0.7736 - recall: 0.6659\n",
      "Epoch 8: val_auc improved from 0.91983 to 0.93836, saving model to checkpoints\\best_weights.h5\n",
      "187/187 [==============================] - 308s 2s/step - loss: 0.6699 - auc: 0.9512 - precision: 0.7736 - recall: 0.6659 - val_loss: 0.8863 - val_auc: 0.9384 - val_precision: 0.7217 - val_recall: 0.6195 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.6115 - auc: 0.9569 - precision: 0.7931 - recall: 0.6729\n",
      "Epoch 9: val_auc improved from 0.93836 to 0.94142, saving model to checkpoints\\best_weights.h5\n",
      "187/187 [==============================] - 305s 2s/step - loss: 0.6115 - auc: 0.9569 - precision: 0.7931 - recall: 0.6729 - val_loss: 0.8560 - val_auc: 0.9414 - val_precision: 0.7415 - val_recall: 0.6346 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5879 - auc: 0.9587 - precision: 0.7911 - recall: 0.6754\n",
      "Epoch 10: val_auc improved from 0.94142 to 0.94983, saving model to checkpoints\\best_weights.h5\n",
      "187/187 [==============================] - 308s 2s/step - loss: 0.5879 - auc: 0.9587 - precision: 0.7911 - recall: 0.6754 - val_loss: 0.7895 - val_auc: 0.9498 - val_precision: 0.7639 - val_recall: 0.6719 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5906 - auc: 0.9591 - precision: 0.7977 - recall: 0.6830\n",
      "Epoch 11: val_auc did not improve from 0.94983\n",
      "187/187 [==============================] - 309s 2s/step - loss: 0.5906 - auc: 0.9591 - precision: 0.7977 - recall: 0.6830 - val_loss: 0.8141 - val_auc: 0.9489 - val_precision: 0.7524 - val_recall: 0.6724 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5665 - auc: 0.9591 - precision: 0.7910 - recall: 0.6828\n",
      "Epoch 12: val_auc improved from 0.94983 to 0.95396, saving model to checkpoints\\best_weights.h5\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "187/187 [==============================] - 309s 2s/step - loss: 0.5665 - auc: 0.9591 - precision: 0.7910 - recall: 0.6828 - val_loss: 0.7583 - val_auc: 0.9540 - val_precision: 0.7772 - val_recall: 0.6946 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5508 - auc: 0.9638 - precision: 0.8122 - recall: 0.6998\n",
      "Epoch 13: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 291s 2s/step - loss: 0.5508 - auc: 0.9638 - precision: 0.8122 - recall: 0.6998 - val_loss: 0.7832 - val_auc: 0.9508 - val_precision: 0.7670 - val_recall: 0.6835 - lr: 4.0000e-05\n",
      "Epoch 14/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5246 - auc: 0.9655 - precision: 0.8136 - recall: 0.7012\n",
      "Epoch 14: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 290s 2s/step - loss: 0.5246 - auc: 0.9655 - precision: 0.8136 - recall: 0.7012 - val_loss: 0.8606 - val_auc: 0.9418 - val_precision: 0.7410 - val_recall: 0.6462 - lr: 4.0000e-05\n",
      "Epoch 15/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5210 - auc: 0.9648 - precision: 0.8095 - recall: 0.7045\n",
      "Epoch 15: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 295s 2s/step - loss: 0.5210 - auc: 0.9648 - precision: 0.8095 - recall: 0.7045 - val_loss: 0.7631 - val_auc: 0.9536 - val_precision: 0.7705 - val_recall: 0.6855 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5083 - auc: 0.9652 - precision: 0.8100 - recall: 0.6998\n",
      "Epoch 16: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 306s 2s/step - loss: 0.5083 - auc: 0.9652 - precision: 0.8100 - recall: 0.6998 - val_loss: 0.7831 - val_auc: 0.9513 - val_precision: 0.7643 - val_recall: 0.6749 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5048 - auc: 0.9650 - precision: 0.8036 - recall: 0.7050\n",
      "Epoch 17: val_auc did not improve from 0.95396\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "187/187 [==============================] - 305s 2s/step - loss: 0.5048 - auc: 0.9650 - precision: 0.8036 - recall: 0.7050 - val_loss: 0.7922 - val_auc: 0.9510 - val_precision: 0.7568 - val_recall: 0.6744 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.4970 - auc: 0.9662 - precision: 0.8127 - recall: 0.7112\n",
      "Epoch 18: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 297s 2s/step - loss: 0.4970 - auc: 0.9662 - precision: 0.8127 - recall: 0.7112 - val_loss: 0.7890 - val_auc: 0.9507 - val_precision: 0.7610 - val_recall: 0.6774 - lr: 8.0000e-06\n",
      "Epoch 19/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5136 - auc: 0.9659 - precision: 0.8134 - recall: 0.7069\n",
      "Epoch 19: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 289s 2s/step - loss: 0.5136 - auc: 0.9659 - precision: 0.8134 - recall: 0.7069 - val_loss: 0.7852 - val_auc: 0.9514 - val_precision: 0.7635 - val_recall: 0.6784 - lr: 8.0000e-06\n",
      "Epoch 20/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5026 - auc: 0.9663 - precision: 0.8171 - recall: 0.7132\n",
      "Epoch 20: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 287s 2s/step - loss: 0.5026 - auc: 0.9663 - precision: 0.8171 - recall: 0.7132 - val_loss: 0.7883 - val_auc: 0.9507 - val_precision: 0.7653 - val_recall: 0.6789 - lr: 8.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.4951 - auc: 0.9666 - precision: 0.8190 - recall: 0.7152\n",
      "Epoch 21: val_auc did not improve from 0.95396\n",
      "187/187 [==============================] - 289s 2s/step - loss: 0.4951 - auc: 0.9666 - precision: 0.8190 - recall: 0.7152 - val_loss: 0.7814 - val_auc: 0.9515 - val_precision: 0.7648 - val_recall: 0.6769 - lr: 8.0000e-06\n",
      "Epoch 22/100\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5033 - auc: 0.9661 - precision: 0.8148 - recall: 0.7060Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 22: val_auc did not improve from 0.95396\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "187/187 [==============================] - 289s 2s/step - loss: 0.5033 - auc: 0.9661 - precision: 0.8148 - recall: 0.7060 - val_loss: 0.7891 - val_auc: 0.9507 - val_precision: 0.7618 - val_recall: 0.6739 - lr: 8.0000e-06\n",
      "Epoch 22: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b257a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'auc', 'precision', 'recall', 'val_loss', 'val_auc', 'val_precision', 'val_recall', 'lr'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
